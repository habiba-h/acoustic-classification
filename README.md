# Acoustic classification
Acoustic Classification is one of the most widely used applications in Audio Deep Learning. It involves learning to classify sounds and to predict the category of that sound. This type of problem can be applied to many scenarios in the wild to track endangered animals e.g. identifying Tinker Frog sounds from audio reordings. 

Research and development in sound classification and recognition is rapidly advancing in the field of pattern recognition. One important area in this field is environmental sound recognition, whether it concerns the identification of endangered species in different habitats or the type of interfering noise in urban environments. However, there are a number of challenges that makes it hard to build such applications in production. Usually the environmental sound data is, challenging to obtain, noisey due to many different types of sounds overlapping each other,   as well as require ad hoc feature engineering task to be used of building deep learning based classification solution. On the other hand, the convolutional neural network (CNN) models are ubiquitous in the image data space. They work phenomenally well on computer vision tasks like image classification, object detection, image recognition, etc. What if we could “see” sound? Turns out there are a number of techniques that can convert sound into images by encoding a number of audio features as image features. Spectrograms are one such technique. A spectrogram is a concise ‘snapshot’ of an audio wave and since it is an image, it is well suited to being input to CNN-based architectures developed for handling images.  Once an audio is converted to an image, we can use one of the many powerful CNN architectures to perform sound classification task as a standard image classification task.
